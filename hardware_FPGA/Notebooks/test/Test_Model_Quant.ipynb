{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized version of the ML model\n",
    "(Non Quantized version of this notebook is done by Shreyas, credit should go to him for the non Quantized model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from load_data import load_dance_data\n",
    "from load_data import one_hot_encoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read from  Hair.txt\n",
      "Files read from  Rocket.txt\n",
      "Files read from  Zig_Zag.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = os.getcwd() + '\\Dance_data'\n",
    "sampling_rate = 10\n",
    "window_size = 2.56\n",
    "\n",
    "X, y = load_dance_data(dataset, sampling_rate, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture cap --no-stderr\n",
    "\n",
    "hair_train = []\n",
    "rocket_train = []\n",
    "zigzag_train = []\n",
    "#print(len(X))\n",
    "for i,j in enumerate(y):\n",
    "    if j == [0]:\n",
    "        hair_train.append(X[i])\n",
    "for i,j in enumerate(y):\n",
    "    if j == [1]:\n",
    "        rocket_train.append(X[i])\n",
    "for i,j in enumerate(y):\n",
    "    if j == [2]:\n",
    "       zigzag_train.append(X[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print  KNN training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of model by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhair_train_int = np.asarray(hair_train, dtype = np.int)\\nrocket_train_int = np.asarray(rocket_train, dtype = np.int)\\nzigzag_train_int = np.asarray(zigzag_train, dtype = np.int)\\nprint(\"int hair_train[\", len(hair_train_int) ,\"][40]\",\\' = {\\' , sep=\\'\\', end=\\'\\')\\nfor i,a in enumerate(hair_train_int):\\n    if i == len(hair_train_int) - 1:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)),\"}};\")\\n    else:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)), \\'},\\')\\n\\nprint()\\nprint()\\nprint()\\nprint(\"int rocket_train_int[\", len(rocket_train_int) ,\"][40]\",\\' = {\\' , sep=\\'\\', end=\\'\\')\\nfor i,a in enumerate(rocket_train_int):\\n    if i == len(rocket_train_int) - 1:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)),\"}};\")\\n    else:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)), \\'},\\')\\nprint()\\nprint()\\nprint()\\nprint(\"int zigzag_train_int[\", len(zigzag_train_int) ,\"][40]\",\\' = {\\' , sep=\\'\\', end=\\'\\')\\nfor i,a in enumerate(zigzag_train_int):\\n    if i == len(zigzag_train_int) - 1:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)),\"}};\")\\n    else:\\n        print(\\'{\\',\\' ,\\'.join(map(str, a)), \\'},\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hair_train_int = np.asarray(hair_train, dtype = np.int)\n",
    "rocket_train_int = np.asarray(rocket_train, dtype = np.int)\n",
    "zigzag_train_int = np.asarray(zigzag_train, dtype = np.int)\n",
    "print(\"int hair_train[\", len(hair_train_int) ,\"][40]\",' = {' , sep='', end='')\n",
    "for i,a in enumerate(hair_train_int):\n",
    "    if i == len(hair_train_int) - 1:\n",
    "        print('{',' ,'.join(map(str, a)),\"}};\")\n",
    "    else:\n",
    "        print('{',' ,'.join(map(str, a)), '},')\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"int rocket_train_int[\", len(rocket_train_int) ,\"][40]\",' = {' , sep='', end='')\n",
    "for i,a in enumerate(rocket_train_int):\n",
    "    if i == len(rocket_train_int) - 1:\n",
    "        print('{',' ,'.join(map(str, a)),\"}};\")\n",
    "    else:\n",
    "        print('{',' ,'.join(map(str, a)), '},')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"int zigzag_train_int[\", len(zigzag_train_int) ,\"][40]\",' = {' , sep='', end='')\n",
    "for i,a in enumerate(zigzag_train_int):\n",
    "    if i == len(zigzag_train_int) - 1:\n",
    "        print('{',' ,'.join(map(str, a)),\"}};\")\n",
    "    else:\n",
    "        print('{',' ,'.join(map(str, a)), '},')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "X_train, X_test = X[:int(len(X)*0.8), :], X[int(len(X)*0.8):, :]\n",
    "y_train, y_test = y[:int(len(X)*0.8), :], y[int(len(X)*0.8):, :]\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "y_train = y_train.reshape(y_train.shape[0]).long()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "y_test = y_test.reshape(y_test.shape[0]).long()\n",
    "\n",
    "input_size = X.shape[1]\n",
    "output_size = one_hot_encoder(y).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_states = [256, 128]\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "model = nn.Sequential(QuantStub(),\n",
    "            nn.Linear(input_size, hidden_states[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states[0], hidden_states[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states[1], output_size),\n",
    "            nn.Sigmoid(),\n",
    "            DeQuantStub())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training and testing\n",
    "import torch.utils.data\n",
    "\n",
    "class Prepare_data(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.inputs[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(Prepare_data(X_train, y_train), batch_size=32, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(Prepare_data(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.8277832199545467\n",
      "Training loss: -0.9878954571836135\n",
      "Training loss: -0.9908285912345437\n",
      "Training loss: -0.9919883714002722\n",
      "Training loss: -0.9927490248399622\n",
      "Training loss: -0.9933833655189065\n",
      "Training loss: -0.9938541510525871\n",
      "Training loss: -0.9941380374571857\n",
      "Training loss: -0.9945162920390859\n",
      "Training loss: -0.9948355169857249\n",
      "Training loss: -0.9950437160099254\n",
      "Training loss: -0.9949472301146564\n",
      "Training loss: -0.9952743474174949\n",
      "Training loss: -0.9950617201188031\n",
      "Training loss: -0.995787006967208\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in trainloader:\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_test)\n",
    "y_pred = y_pred.cpu()\n",
    "result = y_pred.data.numpy()\n",
    "array_res = np.reshape(result, (1, -1))\n",
    "#array_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "test_acc = 0.0\n",
    "count = 0\n",
    "for i, (inputs, labels) in enumerate(testloader):\n",
    "    # Predict classes using images from the test set\n",
    "    outputs = model(inputs)\n",
    "    _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "    test_acc += torch.sum(prediction == labels.data).float()\n",
    "    count += 1\n",
    "\n",
    "# Compute the average acc and loss over all 10000 test images\n",
    "test_acc = test_acc / count\n",
    "int(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77faed59125a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandlers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.handlers import EarlyStopping\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['crossentropy']\n",
    "    accuracy = engine.state.metrics['accuracy']\n",
    "    return accuracy #-val_loss\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, loss)\n",
    "evaluator = create_supervised_evaluator(model,\n",
    "                                        metrics={\n",
    "                                            'accuracy': Accuracy(),\n",
    "                                            'crossentropy': Loss(loss)\n",
    "                                            })\n",
    "\n",
    "handler = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\n",
    "# Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset).\n",
    "evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(trainloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['crossentropy']))\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(testloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['crossentropy']))\n",
    "\n",
    "trainer.run(trainloader, max_epochs=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float inference using inference algorithm\n",
    "\n",
    "Matrix multiplicaition of weights and inputs matrices to get output for linear layers. The float version is compared to the int version of validation and comparison of accuracy, and for debugging purposes as well.\n",
    "\n",
    "For Relu, we simply set the output to zero if it is less than 0; else we do not change it (Hence the name rectified linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\CG4002\\CG4002_B18\\sw1_ml\\src\\data\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neural_network_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-762f15050eb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mneural_network_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneural_network_model_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_network_model_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neural_network_model'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "save_path = os.path.abspath(os.getcwd() + \"/../../sw1_ml/src/model/\")\n",
    "print((save_path))\n",
    "sys.path.insert(1, save_path)\n",
    "from neural_network_model import neural_network_model_1\n",
    "\n",
    "model = neural_network_model_1(40, [256, 128], 3)\n",
    "model.load_state_dict(torch.load(\"NN_v1_3moves.pt\"))\n",
    "#first_model.eval()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.set_printoptions(edgeitems=10000)\\nnp.set_printoptions(edgeitems=10000)\\n\\nlayer_0_float = model[1].weight.data\\nlayer_2_flaot = model[3].weight.data\\nlayer_4_flaot = model[5].weight.data\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.set_printoptions(edgeitems=10000)\n",
    "np.set_printoptions(edgeitems=10000)\n",
    "\n",
    "layer_0_float = model[1].weight.data\n",
    "layer_2_flaot = model[3].weight.data\n",
    "layer_4_flaot = model[5].weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:   Hair Zig-Zag  Hair  Hair Zig-Zag  Hair  Hair Zig-Zag Zig-Zag  Hair  Hair Rocket  Hair Zig-Zag Zig-Zag Zig-Zag  Hair Rocket Zig-Zag Rocket \n",
      "\n",
      "Predicted:     Hair Zig-Zag  Hair  Hair Zig-Zag  Hair  Hair Zig-Zag Zig-Zag  Hair  Hair Zig-Zag  Hair Zig-Zag Zig-Zag Zig-Zag  Hair Zig-Zag Zig-Zag Rocket\n"
     ]
    }
   ],
   "source": [
    "classes = ['Hair', 'Rocket', 'Zig-Zag']\n",
    "testloader = torch.utils.data.DataLoader(Prepare_data(X_test, y_test), batch_size=20)\n",
    "dataiter = iter(testloader)\n",
    "inputs, labels = dataiter.next()\n",
    "\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(len(labels))), '\\n')\n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted:   ', ' '.join('%5s' % classes[predicted[j]] for j in range(len(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_0_float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9ffed22e3b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print (np.array2string(layer_4_flaot.numpy(), separator=', '))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer_0_float\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer_0_float' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "inp = inputs.numpy()\n",
    "#print(inp)\n",
    "layer_0_out_float = []\n",
    "\n",
    "#print (np.array2string(layer_4_flaot.numpy(), separator=', '))\n",
    "\n",
    "for row in layer_0_float:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] * inp[0][col] \n",
    "    layer_0_out_float.append(sum)\n",
    "#layer_1_out = np.cross(layer_1_int, inp)\n",
    "#print(layer_0_out)\n",
    "layer_0_out_float = np.asarray(layer_0_out_float)\n",
    "#print(layer_0_out_float)\n",
    "\n",
    "#relu\n",
    "for i in range(len(layer_0_out_float)):\n",
    "    if layer_0_out_float[i] < 0:\n",
    "        layer_0_out_float[i]  = 0\n",
    "    else:\n",
    "        layer_0_out_float[i]  = layer_0_out_float[i]\n",
    "#print(layer_0_out_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_2_out_float = []\n",
    "#print(layer_0_float)\n",
    "for row in layer_2_flaot:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] * layer_0_out_float[col] \n",
    "    layer_2_out_float.append(sum)\n",
    "#layer_1_out = np.cross(layer_1_int, inp)\n",
    "#print(layer_0_out)\n",
    "layer_2_out_float = np.asarray(layer_2_out_float)\n",
    "#print(layer_0_out)\n",
    "\n",
    "#relu\n",
    "for i in range(len(layer_2_out_float)):\n",
    "    if layer_2_out_float[i] < 0:\n",
    "        layer_2_out_float[i]  = 0\n",
    "    else:\n",
    "        layer_2_out_float[i]  = layer_2_out_float[i]\n",
    "#print(layer_2_out_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_4_out_float = []\n",
    "#print(layer_0_float)\n",
    "for row in layer_4_flaot:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] * layer_2_out_float[col] \n",
    "    layer_4_out_float.append(sum)\n",
    "#layer_1_out = np.cross(layer_1_int, inp)\n",
    "#print(layer_0_out)\n",
    "layer_4_out_float = np.asarray(layer_4_out_float)\n",
    "#print(layer_0_out)\n",
    "\n",
    "#relu\n",
    "for i in range(len(layer_4_out_float)):\n",
    "    if layer_4_out_float[i] < 0:\n",
    "        layer_4_out_float[i]  = 0\n",
    "    else:\n",
    "        layer_4_out_float[i]  = layer_4_out_float[i]\n",
    "#print(layer_4_out_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.Tensor([layer_4_out_float])\n",
    "m =  nn.Sigmoid()\n",
    "out = m(tensor)\n",
    "_, pred = torch.max(out, 1)\n",
    "print(\"Time taken\" ,(time.time() - start_time) * 1000 , 'ms')\n",
    "print(pred)\n",
    "print('Predicted:   ', ' '.join('%5s' % classes[pred[0]]))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(len(labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT INFERENCE\n",
    "### Post-training static quantization\n",
    "Quantisation steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True) \n",
    "\n",
    "evaluator = create_supervised_evaluator(model,\n",
    "                                        metrics={\n",
    "                                            'accuracy': Accuracy(),\n",
    "                                            'crossentropy': Loss(loss)\n",
    "                                            })\n",
    "evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "testloader = torch.utils.data.DataLoader(Prepare_data(X_test, y_test), batch_size=32)\n",
    "evaluator.run(testloader)\n",
    "\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "layer_0_int = (model[1].weight().int_repr().numpy())\n",
    "layer_2_int = (model[3].weight().int_repr().numpy())\n",
    "layer_4_int = (model[5].weight().int_repr().numpy())\n",
    "\n",
    "layer_0_scale = model[1].scale\n",
    "layer_2_scale = model[3].scale\n",
    "layer_4_scale = model[5].scale\n",
    "\n",
    "layer_0_zero = model[1].zero_point\n",
    "layer_2_zero = model[3].zero_point\n",
    "layer_4_zero = model[5].zero_point\n",
    "\n",
    "inp_scale = model[0].scale\n",
    "inp_zero = model[0].zero_point\n",
    "\n",
    "\n",
    "#print(','.join([str(x) for x in inp[0]]))\n",
    "#print((layer_0_int[0])/ layer_0_scale )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp = inputs.numpy().astype(int)\n",
    "print(inp)\n",
    "start_time = time.time()\n",
    "#print(layer_0_int)\n",
    "layer_0_out = []\n",
    "for row in layer_0_int:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] * inp[0][col]\n",
    "    layer_0_out.append(int(sum / inp_scale / layer_0_scale ) )\n",
    "#layer_1_out = np.cross(layer_1_int, inp)\n",
    "#print(layer_0_out)\n",
    "#layer_0_out = np.asarray(layer_0_out)\n",
    "#print(layer_0_out)\n",
    "\n",
    "#relu\n",
    "for i in range(len(layer_0_out)):\n",
    "    if layer_0_out[i] < 0:\n",
    "        layer_0_out[i]  = 0\n",
    "    else:\n",
    "        layer_0_out[i]  = layer_0_out[i] \n",
    "layer_0_out = np.asarray(layer_0_out )\n",
    "#print(layer_0_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_2_in = layer_0_out / layer_0_scale\n",
    "layer_2_out = []\n",
    "for row in layer_2_int:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] * layer_2_in[col]\n",
    "    layer_2_out.append(int(sum / layer_2_scale))\n",
    "#print(layer_2_out)\n",
    "\n",
    "\n",
    "for i in range(len(layer_2_out)):\n",
    "    if layer_2_out[i] < 0:\n",
    "        layer_2_out[i]  = 0\n",
    "\n",
    "layer_2_out = np.asarray(layer_2_out)\n",
    "#print(layer_2_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_4_in = layer_2_out / layer_2_scale\n",
    "layer_4_out = []\n",
    "for row in layer_4_int:\n",
    "    sum = 0\n",
    "    for col in range(len(row)):\n",
    "        sum += row[col] *  layer_4_in[col]\n",
    "    layer_4_out.append(int(sum) / layer_4_scale)       \n",
    "\n",
    "#print(layer_4_scale)    \n",
    "layer_4_out = np.asarray(layer_4_out)\n",
    "#print(layer_4_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.Tensor([layer_4_out])\n",
    "m =  nn.Sigmoid()\n",
    "out = m(tensor)\n",
    "_, pred = torch.max(out, 1)\n",
    "print(\"Time taken\" ,(time.time() - start_time) * 1000 , 'ms')\n",
    "print(pred)\n",
    "print('Predicted:   ', ' '.join('%5s' % classes[pred[0]]))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(len(labels))))\n",
    "\n",
    "print(layer_0_float.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_knn = inputs.numpy()[0]\n",
    "start_time = time.time()\n",
    "hair_dist_min = 10e9\n",
    "rocket_dist_min = 10e9\n",
    "zigzag_dist_min = 10e9\n",
    "\n",
    "for i in range(128):\n",
    "    hair_dist = 0\n",
    "    rocket_dist = 0\n",
    "    zigzag_dist = 0\n",
    "    for j in range(40):\n",
    "        hair_dist += abs(hair_train[i][j] - inputs_knn[j])\n",
    "        rocket_dist += abs(rocket_train[i][j] - inputs_knn[j])\n",
    "        zigzag_dist += abs(zigzag_train[i][j] - inputs_knn[j])\n",
    "    if hair_dist < hair_dist_min:\n",
    "        hair_dist_min = hair_dist\n",
    "    if rocket_dist < rocket_dist_min:\n",
    "        rocket_dist_min = rocket_dist\n",
    "    if zigzag_dist < zigzag_dist_min:\n",
    "        zigzag_dist_min = zigzag_dist\n",
    "print(\"Time taken\" ,(time.time() - start_time) * 1000 , 'ms')        \n",
    "print(hair_dist_min, rocket_dist_min,zigzag_dist_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dump weights to C++ int array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cb2242b1f69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int inputs[40]={\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inp' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "print(\"int inputs[40]={\", end = '')\n",
    "for i in range(len(inp[0])):\n",
    "    if i < len(inp[0]) - 1:\n",
    "        print(inp[0][i], end=',')\n",
    "    else:\n",
    "        print(inp[0][i], end='};')\n",
    "        print()\n",
    "\n",
    "print(\"int inpScale = \" , inp_scale[0].numpy() , ';')    \n",
    "print(\"int scaleMat0 = \" , layer_0_scale , ';')    \n",
    "print(\"int scaleMat1 = \" , layer_2_scale , ';')    \n",
    "print(\"int scaleMat2 = \" , layer_4_scale , ';')    \n",
    "\n",
    "print(\"int linearMat0[256][40] = {\", end = '')\n",
    "for i in layer_0_int:\n",
    "    print('{',end = '')\n",
    "    for j in range(len(i)):\n",
    "        if j < (len(i) - 1):\n",
    "            print(i[j], end = ', ')\n",
    "        else:\n",
    "            print(i[j], end = '')\n",
    "    print('},')\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"int linearMat1[128][256] = {\")\n",
    "for i in layer_2_int:\n",
    "    print('{',end = '')\n",
    "    for j in range(len(i)):\n",
    "        if j < (len(i) - 1):\n",
    "            print(i[j], end = ', ')\n",
    "        else:\n",
    "            print(i[j], end = '')\n",
    "    print('},')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "    \n",
    "print(\"int linearMat2[3][128] = {\")\n",
    "for i in layer_4_int:\n",
    "    print('{',end = '')\n",
    "    for j in range(len(i)):\n",
    "        if j < (len(i) - 1):\n",
    "            print(i[j], end = ', ')\n",
    "        else:\n",
    "            print(i[j], end = '')\n",
    "    print('},')\n",
    "\n",
    "with open('cppWeights.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cppWeights.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
